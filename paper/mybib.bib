@article{turk1991eigenfaces,
  title={Eigenfaces for recognition},
  author={Turk, Matthew and Pentland, Alex},
  journal={Journal of cognitive neuroscience},
  volume={3},
  number={1},
  pages={71--86},
  year={1991},
  publisher={MIT Press}
}

@article{Higuera,
    author = {Higuera, Clara AND Gardiner, Katheleen J. AND Cios, Krzysztof J.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Self-Organizing Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down Syndrome},
    year = {2015},
    month = {06},
    volume = {10},
    url = {https://urldefense.com/v3/__https://doi.org/10.1371/journal.pone.0129126__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhTJbxpYc$ },
    pages = {1-28},
    abstract = {Down syndrome (DS) is a chromosomal abnormality (trisomy of human chromosome 21) associated with intellectual disability and affecting approximately one in 1000 live births worldwide. The overexpression of genes encoded by the extra copy of a normal chromosome in DS is believed to be sufficient to perturb normal pathways and normal responses to stimulation, causing learning and memory deficits. In this work, we have designed a strategy based on the unsupervised clustering method, Self Organizing Maps (SOM), to identify biologically important differences in protein levels in mice exposed to context fear conditioning (CFC). We analyzed expression levels of 77 proteins obtained from normal genotype control mice and from their trisomic littermates (Ts65Dn) both with and without treatment with the drug memantine. Control mice learn successfully while the trisomic mice fail, unless they are first treated with the drug, which rescues their learning ability. The SOM approach identified reduced subsets of proteins predicted to make the most critical contributions to normal learning, to failed learning and rescued learning, and provides a visual representation of the data that allows the user to extract patterns that may underlie novel biological responses to the different kinds of learning and the response to memantine. Results suggest that the application of SOM to new experimental data sets of complex protein profiles can be used to identify common critical protein responses, which in turn may aid in identifying potentially more effective drug targets.},
    number = {6},
    doi = {10.1371/journal.pone.0129126}
}
@article{Ahmed,
    author = {Ahmed, Md. Mahiuddin AND Dhanasekaran, A. Ranjitha AND Block, Aaron AND Tong, Suhong AND Costa, Alberto C. S. AND Stasko, Melissa AND Gardiner, Katheleen J.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Protein Dynamics Associated with Failed and Rescued Learning in the Ts65Dn Mouse Model of Down Syndrome},
    year = {2015},
    month = {03},
    volume = {10},
    url = {https://urldefense.com/v3/__https://doi.org/10.1371/journal.pone.0119491__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhp3Mcfog$ },
    pages = {1-25},
    abstract = {Down syndrome (DS) is caused by an extra copy of human chromosome 21 (Hsa21). Although it is the most common genetic cause of intellectual disability (ID), there are, as yet, no effective pharmacotherapies. The Ts65Dn mouse model of DS is trisomic for orthologs of ∼55 of Hsa21 classical protein coding genes. These mice display many features relevant to those seen in DS, including deficits in learning and memory (L/M) tasks requiring a functional hippocampus. Recently, the N-methyl-D-aspartate (NMDA) receptor antagonist, memantine, was shown to rescue performance of the Ts65Dn in several L/M tasks. These studies, however, have not been accompanied by molecular analyses. In previous work, we described changes in protein expression induced in hippocampus and cortex in control mice after exposure to context fear conditioning (CFC), with and without memantine treatment. Here, we extend this analysis to Ts65Dn mice, measuring levels of 85 proteins/protein modifications, including components of MAP kinase and MTOR pathways, and subunits of NMDA receptors, in cortex and hippocampus of Ts65Dn mice after failed learning in CFC and after learning was rescued by memantine. We show that, compared with wild type littermate controls, (i) of the dynamic responses seen in control mice in normal learning, >40 also occur in Ts65Dn in failed learning or are compensated by baseline abnormalities, and thus are considered necessary but not sufficient for successful learning, and (ii) treatment with memantine does not in general normalize the initial protein levels but instead induces direct and indirect responses in approximately half the proteins measured and results in normalization of the endpoint protein levels. Together, these datasets provide a first view of the complexities associated with pharmacological rescue of learning in the Ts65Dn. Extending such studies to additional drugs and mouse models of DS will aid in identifying pharmacotherapies for effective clinical trials.},
    number = {3},
    doi = {10.1371/journal.pone.0119491}
}
@article{Abid,
    author = {Abid, Abubakar and Zhang, Martin J. and Bagaria, Vivek K. and Zou, James}, 
    journal = {Nature Communications},
    title = {Exploring patterns enriched in a dataset with contrastive principal component analysis},
    year = {2018},
    month = {05},
    volume = {9},
    url = {https://urldefense.com/v3/__https://doi.org/10.1038/s41467-018-04608-8__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhcSs3PrA$ },
    pages = {2134},
    ISBN = {2041-1723},
    abstract = { Visualization and exploration of high-dimensional data is a ubiquitous challenge across disciplines. Widely used techniques such as principal component analysis (PCA) aim to identify dominant trends in one dataset. However, in many settings we have datasets collected under different conditions, e.g., a treatment and a control experiment, and we are interested in visualizing and exploring patterns that are specific to one dataset. This paper proposes a method, contrastive principal component analysis (cPCA), which identifies low-dimensional structures that are enriched in a dataset relative to comparison data. In a wide variety of experiments, we demonstrate that cPCA with a background dataset enables us to visualize dataset-specific patterns missed by PCA and other standard methods. We further provide a geometric interpretation of cPCA and strong mathematical guarantees. An implementation of cPCA is publicly available, and can be used for exploratory data analysis in many applications where PCA is currently used.},
    doi = {10.1038/s41467-018-04608-8}
    }
@article{Golub,
author = {Golub, Gene and Solna, Knut and Van Dooren, Paul},
title = {Computing the SVD of a General Matrix Product/Quotient},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {22},
number = {1},
pages = {1-19},
year = {2000},
doi = {10.1137/S0895479897325578},
URL = {https://urldefense.com/v3/__https://doi.org/10.1137/S0895479897325578__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhD_NUVYk$ },
eprint = {https://urldefense.com/v3/__https://doi.org/10.1137/S0895479897325578__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhD_NUVYk$ }
}
@article{Boileau,
    author = {Boileau, Philippe and Hejazi, Nima S and Dudoit, Sandrine},
    title = "{Exploring high-dimensional biological data with sparse contrastive principal component analysis}",
    journal = {Bioinformatics},
    volume = {36},
    number = {11},
    pages = {3422-3430},
    year = {2020},
    month = {03},
    abstract = "{Statistical analyses of high-throughput sequencing data have re-shaped the biological sciences. In spite of myriad advances, recovering interpretable biological signal from data corrupted by technical noise remains a prevalent open problem. Several classes of procedures, among them classical dimensionality reduction techniques and others incorporating subject-matter knowledge, have provided effective advances. However, no procedure currently satisfies the dual objectives of recovering stable and relevant features simultaneously.Inspired by recent proposals for making use of control data in the removal of unwanted variation, we propose a variant of principal component analysis (PCA), sparse contrastive PCA that extracts sparse, stable, interpretable and relevant biological signal. The new methodology is compared to competing dimensionality reduction approaches through a simulation study and via analyses of several publicly available protein expression, microarray gene expression and single-cell transcriptome sequencing datasets.A free and open-source software implementation of the methodology, the scPCA R package, is made available via the Bioconductor Project. Code for all analyses presented in this article is also available via GitHub.philippe\_boileau@berkeley.eduSupplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btaa176},
    url = {https://urldefense.com/v3/__https://doi.org/10.1093/bioinformatics/btaa176__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhqQBDUSM$ },
    eprint = {https://urldefense.com/v3/__https://academic.oup.com/bioinformatics/article-pdf/36/11/3422/33329159/btaa176.pdf__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrh6vvh0Xk$ },
}
@article{Salloum,
  author    = {Ronald Salloum and
               C.{-}C. Jay Kuo},
  title     = {Efficient Image Splicing Localization via Contrastive Feature Extraction},
  journal   = {CoRR},
  volume    = {abs/1901.07172},
  year      = {2019},
  url       = {https://urldefense.com/v3/__http://arxiv.org/abs/1901.07172__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhlvqh_co$ },
  archivePrefix = {arXiv},
  eprint    = {1901.07172},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://urldefense.com/v3/__https://dblp.org/rec/journals/corr/abs-1901-07172.bib__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhhuULR0w$ },
  bibsource = {dblp computer science bibliography, https://urldefense.com/v3/__https://dblp.org__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhyW1d-uw$ }
}
@Article{Calvo2008,
author={Calvo, Manuel G.
and Lundqvist, Daniel},
title={Facial expressions of emotion (KDEF): Identification under different display-duration conditions},
journal={Behavior Research Methods},
year={2008},
month={Feb},
day={01},
volume={40},
number={1},
pages={109-115},
abstract={Participants judged which of seven facial expressions (neutrality, happiness, anger, sadness, surprise, fear, and disgust) were displayed by a set of 280 faces corresponding to 20 female and 20 male models of the Karolinska Directed Emotional Faces database (Lundqvist, Flykt, {\&} {\"O}hman, 1998). Each face was presented under free-viewing conditions (to 63 participants) and also for 25, SO, 100, 250, and 500 msec (to 160 participants), to examine identification thresholds. Measures of identification accuracy, types of errors, and reaction times were obtained for each expression. In general, happy faces were identified more accurately, earlier, and faster than other faces, whereas judgments of fearful faces were the least accurate, the latest, and the slowest. Norms for each face and expression regarding level of identification accuracy, errors, and reaction times may be downloaded from https://urldefense.com/v3/__http://www.psychonomic.org/archive/__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhNRzyBLY$ .},
issn={1554-3528},
doi={10.3758/BRM.40.1.109},
url={https://urldefense.com/v3/__https://doi.org/10.3758/BRM.40.1.109__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhCNBkfAs$ }
}
@ARTICLE{Fujiwara,
  author={T. {Fujiwara} and O. {Kwon} and K. {Ma}},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning}, 
  year={2020},
  volume={26},
  number={1},
  pages={45-55},}

@ARTICLE{MNIST,  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},}

﻿@Article{rpca,
author={Lenz, Michael
and M{\"u}ller, Franz-Josef
and Zenke, Martin
and Schuppert, Andreas},
title={Principal components analysis and the reported low intrinsic dimensionality of gene expression microarray data},
journal={Scientific Reports},
year={2016},
month={Jun},
day={02},
volume={6},
number={1},
pages={25696},
abstract={Principal components analysis (PCA) is a common unsupervised method for the analysis of gene expression microarray data, providing information on the overall structure of the analyzed dataset. In the recent years, it has been applied to very large datasets involving many different tissues and cell types, in order to create a low dimensional global map of human gene expression. Here, we reevaluate this approach and show that the linear intrinsic dimensionality of this global map is higher than previously reported. Furthermore, we analyze in which cases PCA fails to detect biologically relevant information and point the reader to methods that overcome these limitations. Our results refine the current understanding of the overall structure of gene expression spaces and show that PCA critically depends on the effect size of the biological signal as well as on the fraction of samples containing this signal.},
issn={2045-2322},
doi={10.1038/srep25696},
url={https://urldefense.com/v3/__https://doi.org/10.1038/srep25696__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhCmktN9o$ }
}


@misc{fujiwaraMCA,
    title={Contrastive Multiple Correspondence Analysis (cMCA): Applying the Contrastive Learning Method to Identify Political Subgroups},
    author={Takanori Fujiwara and Tzu-Ping Liu},
    year={2020},
    eprint={2007.04540},
    archivePrefix={arXiv},
    primaryClass={cs.SI}
}
@misc{cautoencoder,
    title={Contrastive Variational Autoencoder Enhances Salient Features},
    author={Abubakar Abid and James Zou},
    year={2019},
    eprint={1902.04601},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{severson2019unsupervised,
  title={Unsupervised learning with contrastive latent variable models},
  author={Severson, Kristen A and Ghosh, Soumya and Ng, Kenney},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={4862--4869},
  year={2019}
}

﻿@Article{Lee1999,
author={Lee, Daniel D.
and Seung, H. Sebastian},
title={Learning the parts of objects by non-negative matrix factorization},
journal={Nature},
year={1999},
month={Oct},
day={01},
volume={401},
number={6755},
pages={788-791},
abstract={Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
issn={1476-4687},
doi={10.1038/44565},
url={https://urldefense.com/v3/__https://doi.org/10.1038/44565__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhgN4aprE$ }
}

@ARTICLE{pp,
  author={J. H. {Friedman} and J. W. {Tukey}},
  journal={IEEE Transactions on Computers}, 
  title={A Projection Pursuit Algorithm for Exploratory Data Analysis}, 
  year={1974},
  volume={C-23},
  number={9},
  pages={881-890},}
  
  @misc{xpca,
    title={XPCA: Extending PCA for a Combination of Discrete and Continuous Variables},
    author={Clifford Anderson-Bergman and Tamara G. Kolda and Kina Kincher-Winoto},
    year={2018},
    eprint={1808.07510},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{ica,
title = "Independent component analysis, A new concept?",
journal = "Signal Processing",
volume = "36",
number = "3",
pages = "287 - 314",
year = "1994",
note = "Higher Order Statistics",
issn = "0165-1684",
doi = "https://urldefense.com/v3/__https://doi.org/10.1016/0165-1684(94)90029-9__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrh3JImS7U$ ",
url = "https://urldefense.com/v3/__http://www.sciencedirect.com/science/article/pii/0165168494900299__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhS9BhvHc$ ",
author = "Pierre Comon",
abstract = "The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.
Zusammenfassung
Die Analyse unabhängiger Komponenten (ICA) eines Vektors beruht auf der Suche nach einer linearen Transformation, die die statistische Abhängigkeit zwischen den Komponenten minimiert. Zur Definition geeigneter Such-Kriterien wird die Entwicklung gemeinsamer Information als Funktion von Kumulanten steigender Ordnung genutzt. Es wird ein effizienter Algorithmus vorgeschlagen, der die Berechnung der ICA für Datenmatrizen innerhalb einer polynomischen Zeit erlaubt. Das Konzept der ICA kann eigentlich als Erweiterung der ‘Principal Component Analysis‘ (PCA) betrachtet werden, die nur die Unabhängigkeit bis zur zweiten Ordnung erzwingen kann und deshalb Richtungen definiert, die orthogonal sind. Potentielle Anwendungen der ICA beinhalten Daten-Analyse und Kompression, Bayes-Detektion, Quellenlokalisierung und blinde Identifikation und Entfaltung.
Résumé
L'Analyse en Composantes Indépendentes (ICA) d'un vecteur aléatoire consiste en la recherche d'une transformation linéaire qui minimise la dépendance statistique entre ses composantes. Afin de définir des critères d'optimisation appropriés, on utilise un développment en série de l'information mutuelle en fonction de cumulants d'ordre croissant. On propose ensuite un algorithme pratique permettant le calcul de l'ICA d'une matrice de données en un temps polynomial. Le concept d'ICA peut être vu en réalité comme une extention de l'Analyse en Composantes Principales (PCA) qui, elle, ne peut imposer l'indépendence qu'au second ordre et définit par conséquent des directions orthogonales. Les applications potentielles de l'ICA incluent l'analyse et la compression de données, la détection bayesienne, la localisation de sources, et l'identification et la déconvolution aveugles."
}
@article{famd,
     author = {Pag\`es, J.},
     title = {Analyse factorielle de donn\'ees mixtes},
     journal = {Revue de Statistique Appliqu\'ee},
     publisher = {Soci\'et\'e fran\c caise de statistique},
     volume = {52},
     number = {4},
     year = {2004},
     pages = {93-111},
     language = {fr},
     url = {https://urldefense.com/v3/__http://www.numdam.org/item/RSA_2004__52_4_93_0__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhUgshGVI$ }
     }
     

@article{byrd1995limited,
  title={A limited memory algorithm for bound constrained optimization},
  author={Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  journal={SIAM Journal on scientific computing},
  volume={16},
  number={5},
  pages={1190--1208},
  year={1995},
  publisher={SIAM}
}



@article{pca,
author = { Karl   Pearson   F.R.S. },
title = {LIII. On lines and planes of closest fit to systems of points in space},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {2},
number = {11},
pages = {559-572},
year  = {1901},
publisher = {Taylor & Francis},
doi = {10.1080/14786440109462720},

URL = { 
        https://urldefense.com/v3/__https://doi.org/10.1080/14786440109462720__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhGuCnQRA$ 
},
eprint = { 
        https://urldefense.com/v3/__https://doi.org/10.1080/14786440109462720__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhGuCnQRA$ 
}
}


%packages
  @Manual{microbenchmark,
    title = {microbenchmark: Accurate Timing Functions},
    author = {Olaf Mersmann},
    year = {2019},
    note = {R package version 1.4-7},
    url = {https://urldefense.com/v3/__https://CRAN.R-project.org/package=microbenchmark__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhupDo7Kw$ },
  }

  @Article{rcpparmadillo,
    title = {RcppArmadillo: Accelerating R with high-performance C++ linear algebra},
    author = {Dirk Eddelbuettel and Conrad Sanderson},
    journal = {Computational	Statistics and Data Analysis},
    year = {2014},
    volume = {71},
    month = {March},
    pages = {1054--1063},
    url = {https://urldefense.com/v3/__http://dx.doi.org/10.1016/j.csda.2013.02.005__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhWPJ7v18$ },
  }
  @Manual{Rspectra,
    title = {RSpectra: Solvers for Large-Scale Eigenvalue and SVD Problems},
    author = {Yixuan Qiu and Jiali Mei},
    year = {2019},
    note = {R package version 0.16-0},
    url = {https://urldefense.com/v3/__https://CRAN.R-project.org/package=RSpectra__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhxBmOLmM$ },
  }
    @Manual{baseR,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2020},
    url = {https://urldefense.com/v3/__https://www.R-project.org/__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhjStqGrc$ },
  }
  
  @article{Nguyen2019,
    author = {Nguyen, Lan Huong AND Holmes, Susan},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Ten quick tips for effective dimensionality reduction},
    year = {2019},
    month = {06},
    volume = {15},
    url = {https://urldefense.com/v3/__https://doi.org/10.1371/journal.pcbi.1006907__;!!DZ3fjg!sGIYM1o_8SSJW1QbOZ62mPidAZTlpAuO5P4v-6IHek6bOPrBOHX6LTxlOlrhiJrvh8o$ },
    pages = {1-19},
    abstract = {},
    number = {6},
    doi = {10.1371/journal.pcbi.1006907}
}